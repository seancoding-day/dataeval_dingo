# Dingo å¹»è§‰æ£€æµ‹åŠŸèƒ½å®Œæ•´æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•åœ¨ Dingo ä¸­ä½¿ç”¨é›†æˆçš„å¹»è§‰æ£€æµ‹åŠŸèƒ½ï¼Œæ”¯æŒä¸¤ç§æ£€æµ‹æ–¹æ¡ˆï¼š**HHEM-2.1-Open æœ¬åœ°æ¨¡å‹**ï¼ˆæ¨èï¼‰å’Œ **GPT-based äº‘ç«¯æ£€æµ‹**ã€‚

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

å¹»è§‰æ£€æµ‹åŠŸèƒ½ç”¨äºè¯„ä¼° LLM ç”Ÿæˆçš„å›ç­”æ˜¯å¦ä¸æä¾›çš„å‚è€ƒä¸Šä¸‹æ–‡å­˜åœ¨äº‹å®æ€§çŸ›ç›¾ã€‚ç‰¹åˆ«é€‚ç”¨äºï¼š

- **RAG ç³»ç»Ÿè¯„ä¼°**: æ£€æµ‹ç”Ÿæˆå›ç­”ä¸æ£€ç´¢æ–‡æ¡£çš„ä¸€è‡´æ€§
- **SFT æ•°æ®è´¨é‡è¯„ä¼°**: éªŒè¯è®­ç»ƒæ•°æ®ä¸­å›ç­”çš„äº‹å®å‡†ç¡®æ€§
- **LLM è¾“å‡ºéªŒè¯**: å®æ—¶æ£€æµ‹æ¨¡å‹è¾“å‡ºä¸­çš„å¹»è§‰é—®é¢˜

## ğŸ”§ æ ¸å¿ƒåŸç†

### è¯„ä¼°æµç¨‹

1. **æ•°æ®å‡†å¤‡**: æä¾›å¾…æ£€æµ‹å›ç­”å’Œå‚è€ƒä¸Šä¸‹æ–‡
2. **ä¸€è‡´æ€§åˆ†æ**: åˆ¤æ–­å›ç­”æ˜¯å¦ä¸æ¯ä¸ªä¸Šä¸‹æ–‡ä¸€è‡´
3. **åˆ†æ•°è®¡ç®—**: è®¡ç®—æ•´ä½“å¹»è§‰åˆ†æ•°
4. **é˜ˆå€¼åˆ¤æ–­**: æ ¹æ®è®¾å®šé˜ˆå€¼å†³å®šæ˜¯å¦æ ‡è®°ä¸ºæœ‰é—®é¢˜

### è¯„åˆ†æœºåˆ¶

- **åˆ†æ•°èŒƒå›´**: 0.0 - 1.0
- **åˆ†æ•°å«ä¹‰**:
  - 0.0 = å®Œå…¨æ— å¹»è§‰
  - 1.0 = å®Œå…¨å¹»è§‰
- **é»˜è®¤é˜ˆå€¼**: 0.5 ï¼ˆå¯é…ç½®ï¼‰


## ğŸ“‹ ä½¿ç”¨è¦æ±‚

### æ•°æ®æ ¼å¼è¦æ±‚

```python
from dingo.io.input import Data

data = Data(
    data_id="test_1",
    prompt="ç”¨æˆ·çš„é—®é¢˜",  # åŸå§‹é—®é¢˜ï¼ˆå¯é€‰ï¼‰
    content="LLMçš„å›ç­”",  # éœ€è¦æ£€æµ‹çš„å›ç­”
    context=["å‚è€ƒä¸Šä¸‹æ–‡1", "å‚è€ƒä¸Šä¸‹æ–‡2"]  # å‚è€ƒä¸Šä¸‹æ–‡ï¼ˆå¿…éœ€ï¼‰
)
```

### æ”¯æŒçš„ä¸Šä¸‹æ–‡æ ¼å¼

```python
# æ–¹å¼1: å­—ç¬¦ä¸²åˆ—è¡¨
context = ["ä¸Šä¸‹æ–‡1", "ä¸Šä¸‹æ–‡2", "ä¸Šä¸‹æ–‡3"]

# æ–¹å¼2: JSONå­—ç¬¦ä¸²
context = '["ä¸Šä¸‹æ–‡1", "ä¸Šä¸‹æ–‡2", "ä¸Šä¸‹æ–‡3"]'

# æ–¹å¼3: å•ä¸ªå­—ç¬¦ä¸²
context = "å•ä¸ªå‚è€ƒä¸Šä¸‹æ–‡"
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•ä¸€ï¼šHHEM-2.1-Open æœ¬åœ°æ¨¡å‹ï¼ˆæ¨èï¼‰

#### å®‰è£…ä¾èµ–
```bash
pip install transformers torch
# æˆ–ä½¿ç”¨ä¸“é—¨çš„ä¾èµ–æ–‡ä»¶
pip install -r requirements/hhem_integration.txt
```

#### åŸºæœ¬ä½¿ç”¨

```python
from dingo.io.input import Data
from dingo.model.rule.rule_hallucination_hhem import RuleHallucinationHHEM

# å‡†å¤‡æµ‹è¯•æ•°æ®
data = Data(
    data_id='test_1',
    prompt="çˆ±å› æ–¯å¦ä»€ä¹ˆæ—¶å€™è·å¾—è¯ºè´å°”å¥–ï¼Ÿ",
    content="çˆ±å› æ–¯å¦åœ¨1969å¹´å› å‘ç°å…‰ç”µæ•ˆåº”è·å¾—è¯ºè´å°”å¥–ã€‚",
    context=[
        "çˆ±å› æ–¯å¦å› å‘ç°å…‰ç”µæ•ˆåº”è·å¾—è¯ºè´å°”å¥–ã€‚",
        "çˆ±å› æ–¯å¦åœ¨1921å¹´è·å¾—è¯ºè´å°”å¥–ã€‚"
    ]
)

# æ‰§è¡Œæ£€æµ‹ï¼ˆæ— éœ€APIå¯†é’¥ï¼Œæœ¬åœ°æ¨ç†ï¼‰
result = RuleHallucinationHHEM.eval(data)

# æŸ¥çœ‹ç»“æœ
print(f"æ˜¯å¦æ£€æµ‹åˆ°å¹»è§‰: {result.error_status}")
print(f"HHEM åˆ†æ•°: {getattr(result, 'score', 'N/A')}")
print(f"è¯¦ç»†åˆ†æ: {result.reason[0]}")
```

### æ–¹æ³•äºŒï¼šGPT-based äº‘ç«¯æ£€æµ‹

```python
from dingo.config.input_args import EvaluatorLLMArgs
from dingo.io.input import Data
from dingo.model.llm.llm_hallucination import LLMHallucination

# é…ç½® LLM
LLMHallucination.dynamic_config = EvaluatorLLMArgs(
    key='YOUR_OPENAI_API_KEY',
    api_url='https://api.openai.com/v1/chat/completions',
    model='gpt-4o',
)

# å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆåŒä¸Šï¼‰
data = Data(
    data_id='test_1',
    prompt="çˆ±å› æ–¯å¦ä»€ä¹ˆæ—¶å€™è·å¾—è¯ºè´å°”å¥–ï¼Ÿ",
    content="çˆ±å› æ–¯å¦åœ¨1969å¹´å› å‘ç°å…‰ç”µæ•ˆåº”è·å¾—è¯ºè´å°”å¥–ã€‚",
    context=[
        "çˆ±å› æ–¯å¦å› å‘ç°å…‰ç”µæ•ˆåº”è·å¾—è¯ºè´å°”å¥–ã€‚",
        "çˆ±å› æ–¯å¦åœ¨1921å¹´è·å¾—è¯ºè´å°”å¥–ã€‚"
    ]
)

# æ‰§è¡Œæ£€æµ‹
result = LLMHallucination.eval(data)

# æŸ¥çœ‹ç»“æœ
print(f"æ˜¯å¦æ£€æµ‹åˆ°å¹»è§‰: {result.error_status}")
print(f"å¹»è§‰åˆ†æ•°: {getattr(result, 'score', 'N/A')}")
print(f"è¯¦ç»†åŸå› : {result.reason[0]}")
```

## ğŸ“Š æ‰¹é‡æ•°æ®é›†è¯„ä¼°

### ä½¿ç”¨ HHEM-2.1-Openï¼ˆæœ¬åœ°ï¼Œå…è´¹ï¼‰

```python
from dingo.config import InputArgs
from dingo.exec import Executor

input_data = {
    "input_path": "your_dataset.jsonl",
    "data_format": "jsonl",
    "dataset": "local",
    "custom_config": {
        "rule_config": {
            "RuleHallucinationHHEM": {  # ä½¿ç”¨ HHEM æ¨¡å‹
                "threshold": 0.5
            }
        }
    },
    "save_data": True,
    "output_dir": "output/hhem_hallucination_check"
}

input_args = InputArgs(**input_data)
executor = Executor.exec_map["local"](input_args)
result = executor.execute()

print(f"HHEM å¹»è§‰æ£€æµ‹å®Œæˆ: å‘ç° {result.bad_count}/{result.total_count} ä¸ªé—®é¢˜")
```

### ä½¿ç”¨ GPTï¼ˆåœ¨çº¿ï¼Œéœ€è¦ APIï¼‰

```python
from dingo.config import InputArgs
from dingo.exec import Executor

input_data = {
    "input_path": "your_dataset.jsonl",
    "data_format": "jsonl",
    "dataset": "local",
    "eval_group": "sft",  # ä½¿ç”¨åŒ…å«å¹»è§‰æ£€æµ‹çš„è¯„ä¼°ç»„
    "custom_config": {
        "prompt_list": ["QUALITY_BAD_HALLUCINATION"],  # ä»…è¿è¡Œå¹»è§‰æ£€æµ‹
        "llm_config": {
            "LLMHallucination": {
                "model": "gpt-4o",
                "key": "YOUR_OPENAI_API_KEY",
                "api_url": "https://api.openai.com/v1/chat/completions"
            }
        }
    },
    "save_data": True,
    "output_dir": "output/gpt_hallucination_check"
}

input_args = InputArgs(**input_data)
executor = Executor.exec_map["local"](input_args)
result = executor.execute()

print(f"GPT å¹»è§‰æ£€æµ‹å®Œæˆ: å‘ç° {result.bad_count}/{result.total_count} ä¸ªé—®é¢˜")
```

## ğŸ›ï¸ é«˜çº§é…ç½®

### è‡ªå®šä¹‰é˜ˆå€¼

```python
# æ–¹å¼1: ç›´æ¥è®¾ç½®ç±»å±æ€§
RuleHallucinationHHEM.dynamic_config.threshold = 0.3  # HHEM æ›´ä¸¥æ ¼çš„æ£€æµ‹
LLMHallucination.threshold = 0.3      # GPT æ›´ä¸¥æ ¼çš„æ£€æµ‹

# æ–¹å¼2: é€šè¿‡é…ç½®æ–‡ä»¶
{
    "rule_config": {
        "RuleHallucinationHHEM": {
            "threshold": 0.7  # æ›´å®½æ¾çš„æ£€æµ‹
        }
        },
    "llm_config": {
        "LLMHallucination": {
            "model": "gpt-4o",
            "key": "YOUR_API_KEY",
            "api_url": "https://api.openai.com/v1/chat/completions",
            "threshold": 0.7  # æ›´å®½æ¾çš„æ£€æµ‹
        }
    }
}
```

### é˜ˆå€¼å»ºè®®

- **ä¸¥æ ¼æ£€æµ‹** (0.2-0.3): ç”¨äºé«˜è´¨é‡è¦æ±‚çš„ç”Ÿäº§ç¯å¢ƒ
- **å¹³è¡¡æ£€æµ‹** (0.4-0.6): ç”¨äºä¸€èˆ¬è´¨é‡æ§åˆ¶
- **å®½æ¾æ£€æµ‹** (0.7-0.8): ç”¨äºåˆæ­¥ç­›é€‰æˆ–å®½å®¹åœºæ™¯

### æ€§èƒ½ä¼˜åŒ–é…ç½®

```python
# HHEM æ‰¹é‡å¤„ç†ä¼˜åŒ–
RuleHallucinationHHEM.load_model()  # é¢„åŠ è½½æ¨¡å‹
results = RuleHallucinationHHEM.batch_evaluate(data_list)  # æ‰¹é‡æ›´é«˜æ•ˆ

# GPT å¤šæ¨¡å‹é…ç½®
{
    "custom_config": {
        "prompt_list": [
            "QUALITY_BAD_HALLUCINATION",
            "QUALITY_HELPFUL",
            "QUALITY_HARMLESS"
        ],
        "llm_config": {
            "LLMHallucination": {
                "model": "gpt-4o",
                "key": "YOUR_API_KEY",
                "api_url": "https://api.openai.com/v1/chat/completions"
            },
            "LLMText3HHelpful": {
                "model": "gpt-4o-mini",  # ä½¿ç”¨ä¸åŒæ¨¡å‹
                "key": "YOUR_API_KEY",
                "api_url": "https://api.openai.com/v1/chat/completions"
            }
        }
    }
}
```

## ğŸ“Š è¾“å‡ºç»“æœè§£æ

### ModelRes å­—æ®µè¯´æ˜

```python
result = RuleHallucinationHHEM.eval(data)  # æˆ– LLMHallucination.eval(data)

# æ ‡å‡†å­—æ®µ
result.error_status      # bool: æ˜¯å¦æ£€æµ‹åˆ°å¹»è§‰
result.type             # str: è´¨é‡ç±»å‹æ ‡è¯†
result.name             # str: æ£€æµ‹ç»“æœåç§°
result.reason           # List[str]: è¯¦ç»†åˆ†æåŸå› 

# æ‰©å±•å­—æ®µ
result.score            # float: å¹»è§‰åˆ†æ•° (0.0-1.0)
result.verdict_details  # List[str]: æ¯ä¸ªä¸Šä¸‹æ–‡çš„åˆ¤æ–­è¯¦æƒ…ï¼ˆGPT æ¨¡å¼ï¼‰
result.consistency_scores # List[float]: HHEM åŸå§‹ä¸€è‡´æ€§åˆ†æ•°ï¼ˆHHEM æ¨¡å¼ï¼‰
```

### å…¸å‹è¾“å‡ºç¤ºä¾‹

#### HHEM è¾“å‡ºç¤ºä¾‹
```
HHEM å¹»è§‰åˆ†æ•°: 0.650 (é˜ˆå€¼: 0.500)
å¤„ç†äº† 2 ä¸ªä¸Šä¸‹æ–‡å¯¹ï¼š
  1. ä¸Šä¸‹æ–‡: "çˆ±å› æ–¯å¦å› å‘ç°å…‰ç”µæ•ˆåº”è·å¾—è¯ºè´å°”å¥–ã€‚"
     ä¸€è‡´æ€§: 0.95 â†’ å¹»è§‰åˆ†æ•°: 0.05
  2. ä¸Šä¸‹æ–‡: "çˆ±å› æ–¯å¦åœ¨1921å¹´è·å¾—è¯ºè´å°”å¥–ã€‚"
     ä¸€è‡´æ€§: 0.35 â†’ å¹»è§‰åˆ†æ•°: 0.65
å¹³å‡å¹»è§‰åˆ†æ•°: 0.350
âŒ æ£€æµ‹åˆ°å¹»è§‰: è¶…è¿‡é˜ˆå€¼ 0.500
```

#### GPT è¾“å‡ºç¤ºä¾‹
```
å¹»è§‰åˆ†æ•°: 0.500 (é˜ˆå€¼: 0.500)
å‘ç° 1 ä¸ªçŸ›ç›¾:
  1. å®é™…è¾“å‡ºä¸æä¾›çš„ä¸Šä¸‹æ–‡çŸ›ç›¾ï¼Œä¸Šä¸‹æ–‡è¯´çˆ±å› æ–¯å¦åœ¨1921å¹´è·å¾—è¯ºè´å°”å¥–ï¼Œè€Œä¸æ˜¯1969å¹´ã€‚
å‘ç° 1 ä¸ªäº‹å®ä¸€è‡´:
  1. å®é™…è¾“å‡ºä¸æä¾›çš„ä¸Šä¸‹æ–‡ä¸€è‡´ï¼Œéƒ½è¯´çˆ±å› æ–¯å¦å› å‘ç°å…‰ç”µæ•ˆåº”è·å¾—è¯ºè´å°”å¥–ã€‚
âŒ æ£€æµ‹åˆ°å¹»è§‰: å›ç­”åŒ…å«äº‹å®æ€§çŸ›ç›¾
```

## ğŸ“ æ•°æ®é›†æ ¼å¼è¦æ±‚

### JSONL æ ¼å¼ç¤ºä¾‹

```jsonl
{"data_id": "1", "prompt": "é—®é¢˜", "content": "å›ç­”", "context": ["ä¸Šä¸‹æ–‡1", "ä¸Šä¸‹æ–‡2"]}
{"data_id": "2", "prompt": "é—®é¢˜", "content": "å›ç­”", "context": "å•ä¸ªä¸Šä¸‹æ–‡"}
```

### è‡ªå®šä¹‰åˆ—å

```python
{
    "column_content": "generated_response",  # LLMç”Ÿæˆçš„å›ç­”
    "column_prompt": "user_question",       # ç”¨æˆ·é—®é¢˜
    "column_context": "retrieved_docs",     # æ£€ç´¢åˆ°çš„æ–‡æ¡£
    "column_id": "question_id"              # æ•°æ®ID
}
```

## ğŸ” å…¸å‹åº”ç”¨åœºæ™¯

### 1. RAG ç³»ç»Ÿè´¨é‡ç›‘æ§

```python
# å®æ—¶åŸºäºRAGç›‘æ§å›ç­”è´¨é‡ï¼ˆä½¿ç”¨æœ¬åœ°HHEMï¼‰
def monitor_rag_response(question, generated_answer, retrieved_docs):
    data = Data(
        data_id=f"rag_{timestamp}",
        prompt=question,
        content=generated_answer,
        context=retrieved_docs
    )

    result = RuleHallucinationHHEM.eval(data)  # æœ¬åœ°ã€å¿«é€Ÿã€å…è´¹

    if result.error_status:
        logger.warning(f"æ£€æµ‹åˆ°å¹»è§‰: {result.reason[0]}")
        # è§¦å‘äººå·¥å®¡æ ¸æˆ–å›ç­”é‡ç”Ÿæˆ
```

### 2. SFT æ•°æ®é›†é¢„å¤„ç†

```python
# è®­ç»ƒå‰æ£€æŸ¥SFTæ•°æ®è´¨é‡ï¼ˆæ‰¹é‡å¤„ç†ä½¿ç”¨HHEMï¼‰
input_data = {
    "input_path": "sft_training_data.jsonl",
    "custom_config": {
        "rule_config": {"RuleHallucinationHHEM": {"threshold": 0.4}}
    },
    "save_correct": True,  # ä¿å­˜é€šè¿‡æ£€æµ‹çš„æ•°æ®ç”¨äºè®­ç»ƒ
}
```

### 3. æ¨¡å‹è¾“å‡ºåå¤„ç†

```python
# ç”Ÿäº§ç¯å¢ƒä¸­è¿‡æ»¤æœ‰é—®é¢˜çš„å›ç­”
def filter_hallucinated_responses(responses_with_context):
    clean_responses = []

    for item in responses_with_context:
        data = Data(**item)
        # ä½¿ç”¨æœ¬åœ°HHEMè¿›è¡Œå¿«é€Ÿæ£€æµ‹
        result = RuleHallucinationHHEM.eval(data)

        if not result.error_status:  # æ— å¹»è§‰
            clean_responses.append(item)
        else:
            log_quality_issue(item, result.reason[0])

    return clean_responses
```

### 4. ä¼ä¸šçº§éƒ¨ç½²

```python
# å®Œæ•´çš„ä¼ä¸šçº§RAGç³»ç»Ÿï¼ˆé›†æˆæ£€ç´¢+ç”Ÿæˆ+å¹»è§‰æ£€æµ‹ï¼‰
class RAGWithHallucinationDetection:
    def __init__(self, retriever, llm, hallucination_detector):
        self.retriever = retriever
        self.llm = llm
        self.detector = hallucination_detector
        # é¢„åŠ è½½HHEMæ¨¡å‹ä»¥æé«˜æ€§èƒ½
        self.detector.load_model()

    def generate_answer(self, question):
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.retriever.search(question, top_k=3)

        # 2. ç”Ÿæˆå›ç­”
        context = "\n".join(retrieved_docs)
        prompt = f"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜:\n{context}\n\né—®é¢˜: {question}\nå›ç­”:"
        generated_answer = self.llm.generate(prompt)

        # 3. å¹»è§‰æ£€æµ‹
        data = Data(
            data_id=generate_id(),
            prompt=question,
            content=generated_answer,
            context=retrieved_docs  # æ£€ç´¢åˆ°çš„åŸå§‹æ–‡æ¡£
        )

        hallucination_result = self.detector.eval(data)

        # 4. æ ¹æ®æ£€æµ‹ç»“æœå†³å®šæ˜¯å¦è¿”å›ç­”æ¡ˆ
        if hallucination_result.error_status:
            self.log_hallucination(question, generated_answer, hallucination_result)
            return {
                "answer": None,
                "warning": "æ£€æµ‹åˆ°æ½œåœ¨å¹»è§‰ï¼Œè¯·äººå·¥å®¡æ ¸",
                "retrieved_docs": retrieved_docs,
                "hallucination_score": getattr(hallucination_result, 'score', 'N/A')
            }
        else:
            return {
                "answer": generated_answer,
                "retrieved_docs": retrieved_docs,
                "confidence": "high"
            }

    def log_hallucination(self, question, answer, result):
        # è®°å½•å¹»è§‰æ£€æµ‹ç»“æœç”¨äºç³»ç»Ÿä¼˜åŒ–
        logger.warning(f"å¹»è§‰æ£€æµ‹è­¦å‘Š: {result.reason[0]}")

# ä½¿ç”¨ç¤ºä¾‹
rag_system = RAGWithHallucinationDetection(
    retriever=VectorRetriever("knowledge_base"),
    llm=OpenAILLM("gpt-4"),
    detector=RuleHallucinationHHEM
)

result = rag_system.generate_answer("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶ç»“æ„

```
dingo/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ llm_hallucination.py            # GPT-based æ£€æµ‹ï¼ˆDeepEvalé£æ ¼ï¼‰
â”‚   â”œâ”€â”€ rule/
â”‚   â”‚   â””â”€â”€ rule_hallucination_hhem.py      # HHEM-2.1-Open é›†æˆ
â”‚   â”œâ”€â”€ prompt/prompt_hallucination.py       # GPT æç¤ºè¯æ¨¡æ¿
â”‚   â””â”€â”€ response/response_hallucination.py   # å“åº”æ•°æ®ç»“æ„
â”œâ”€â”€ io/input/Data.py                         # æ‰©å±•Dataç±»æ”¯æŒcontext
â”œâ”€â”€ examples/hallucination/                  # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ sdk_rule_hhem_detection.py          # Rule-based HHEM ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ sdk_hallucination_detection.py      # GPT ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ dataset_hallucination_evaluation.py # æ‰¹é‡è¯„ä¼°ç¤ºä¾‹
â””â”€â”€ requirements/hhem_integration.txt        # HHEM ä¾èµ–
```
