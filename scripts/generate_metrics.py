#!/usr/bin/env python3
"""
è‡ªåŠ¨ç”Ÿæˆ Metrics æ–‡æ¡£çš„è„šæœ¬
æŒ‰ç…§ 3H Assessment Prompts è¡¨æ ¼çš„æ ¼å¼ç”Ÿæˆæ–‡æ¡£
"""

import sys
from pathlib import Path
from typing import Any, Dict, List

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from dingo.model.model import Model  # noqa: E402


def scan_prompt_classes() -> List[Dict[str, Any]]:
    """æ‰«ææ‰€æœ‰ prompt ç±»ï¼Œæå– _metric_info ä¿¡æ¯"""
    # å…ˆåŠ è½½æ¨¡å‹
    Model.load_model()

    metrics_info = []

    # ç›´æ¥ä» prompt_metric_type_map ä¸­è·å–ä¿¡æ¯
    for metric_type, prompt_classes in Model.prompt_metric_type_map.items():
        for prompt_class in prompt_classes:
            if hasattr(prompt_class, '_metric_info'):
                info = prompt_class._metric_info.copy()
                info['prompt_type'] = metric_type
                info['class_name'] = prompt_class.__name__
                info['type'] = 'prompt'
                metrics_info.append(info)

    return metrics_info


def scan_rule_classes() -> List[Dict[str, Any]]:
    """æ‰«ææ‰€æœ‰ rule ç±»ï¼Œæå– _metric_info ä¿¡æ¯"""
    # å…ˆåŠ è½½æ¨¡å‹
    Model.load_model()

    metrics_info = []

    # ç›´æ¥ä» rule_metric_type_map ä¸­è·å–ä¿¡æ¯
    for metric_type, rule_classes in Model.rule_metric_type_map.items():
        for rule_class in rule_classes:
            if hasattr(rule_class, '_metric_info'):
                info = rule_class._metric_info.copy()
                info['rule_type'] = metric_type
                info['class_name'] = rule_class.__name__
                info['type'] = 'rule'

                # å¦‚æœ _metric_info ä¸­æ²¡æœ‰è®¾ç½® categoryï¼Œåˆ™æ ¹æ®ç±»å‹è®¾ç½®é»˜è®¤å€¼
                if 'category' not in info or not info['category']:
                    info['category'] = 'Rule-Based Quality Metrics'

                metrics_info.append(info)

    return metrics_info


def truncate_description(description: str, max_length: int = 120) -> str:
    """æˆªæ–­descriptionåˆ°æŒ‡å®šé•¿åº¦"""
    if len(description) <= max_length:
        return description
    return description[:max_length - 3] + "..."


def generate_table_section(title: str, metrics: List[Dict[str, Any]]) -> str:
    """ç”Ÿæˆè¡¨æ ¼éƒ¨åˆ†"""
    if not metrics:
        return ""

    # è¡¨æ ¼å¤´éƒ¨
    table = f"### {title}\n\n"
    table += "| Type | Metric | Description | Paper Source | Evaluation Results |\n"
    table += "|------|--------|-------------|--------------|-------------------|\n"

    # å¯¹äºruleç±»ï¼ŒæŒ‰typeåˆ†ç»„åˆå¹¶ï¼›å¯¹äºpromptç±»ï¼Œä¿æŒåŸæœ‰é€»è¾‘
    if title.startswith("Rule-Based") and "Quality Metrics" in title:
        # æŒ‰typeåˆ†ç»„
        type_groups = {}
        for metric in metrics:
            if metric.get('type') == 'rule':
                rule_type = metric.get('rule_type', '')
                if rule_type not in type_groups:
                    type_groups[rule_type] = []
                type_groups[rule_type].append(metric)

        # ä¸ºæ¯ä¸ªtypeç”Ÿæˆä¸€è¡Œ
        for rule_type in sorted(type_groups.keys()):
            group_metrics = type_groups[rule_type]
            type_name = f"`{rule_type}`"

            # åˆå¹¶åŒä¸€typeçš„metricåç§°
            metric_names = [m['class_name'] for m in group_metrics]
            combined_metrics = ", ".join(metric_names)

            # åˆå¹¶æè¿°ï¼ˆå–ç¬¬ä¸€ä¸ªä½œä¸ºä»£è¡¨ï¼Œæˆ–è€…åˆå¹¶æ‰€æœ‰æè¿°ï¼‰
            descriptions = [m['description'] for m in group_metrics]
            combined_description = "; ".join(descriptions)
            combined_description = truncate_description(combined_description)

            # å–ç¬¬ä¸€ä¸ªmetricçš„è®ºæ–‡ä¿¡æ¯ï¼ˆå› ä¸ºéƒ½æ˜¯ç›¸åŒçš„ï¼‰
            first_metric = group_metrics[0]

            # å¤„ç†è®ºæ–‡æ¥æº
            if first_metric.get('paper_url') and first_metric.get('paper_title'):
                paper_urls = [url.strip() for url in first_metric['paper_url'].split(',')]
                paper_titles = [title.strip() for title in first_metric['paper_title'].split('&')]

                # å¦‚æœæœ‰å¤šä¸ªURLå’Œæ ‡é¢˜ï¼Œä¸ºæ¯ä¸ªåˆ›å»ºå•ç‹¬çš„é“¾æ¥
                if len(paper_urls) > 1 and len(paper_titles) > 1:
                    links = []
                    for i, (title, url) in enumerate(zip(paper_titles, paper_urls)):
                        links.append(f"[{title}]({url})")
                    paper_source = " & ".join(links)
                else:
                    paper_source = f"[{first_metric['paper_title']}](" \
                        f"{first_metric['paper_url']})"

                if first_metric.get('paper_authors'):
                    paper_source += f" ({first_metric['paper_authors']})"
            else:
                paper_source = "Internal Implementation"

            # å¤„ç†è¯„æµ‹ç»“æœ
            if first_metric.get('evaluation_results'):
                # ä¿®æ­£ç›¸å¯¹è·¯å¾„ï¼šä» docs/metrics.md åˆ° docs/eval/prompt/xxx.md
                eval_path = first_metric['evaluation_results']
                if eval_path.startswith('docs/'):
                    eval_path = eval_path[5:]  # å»æ‰ 'docs/' å‰ç¼€
                eval_results = f"[ğŸ“Š See Results]({eval_path})"
            else:
                eval_results = "N/A"

            table += f"| {type_name} | {combined_metrics} | " \
                f"{combined_description} | {paper_source} | {eval_results} |\n"
    else:
        # å¯¹äºpromptç±»ï¼Œä¿æŒåŸæœ‰é€»è¾‘
        sort_key = lambda x: x.get('prompt_type', x.get('rule_type', ''))  # noqa: E731
        for metric in sorted(metrics, key=sort_key):
            # å¤„ç†typeåˆ—
            if metric.get('type') == 'prompt':
                type_name = f"`{metric['prompt_type']}`"
            elif metric.get('type') == 'rule':
                type_name = f"`{metric['rule_type']}`"
            else:
                type_name = "N/A"

            # å¯¹äºruleç±»ï¼Œä½¿ç”¨ç±»åä½œä¸ºmetricåç§°ï¼›å¯¹äºpromptç±»ï¼Œä½¿ç”¨æè¿°åç§°
            if metric.get('type') == 'rule':
                metric_name = metric['class_name']
            else:
                metric_name = metric['metric_name']
            description = truncate_description(metric['description'])

            # å¤„ç†è®ºæ–‡æ¥æº
            if metric.get('paper_url') and metric.get('paper_title'):
                paper_urls = [url.strip() for url in metric['paper_url'].split(',')]
                paper_titles = [title.strip() for title in metric['paper_title'].split('&')]

                # å¦‚æœæœ‰å¤šä¸ªURLå’Œæ ‡é¢˜ï¼Œä¸ºæ¯ä¸ªåˆ›å»ºå•ç‹¬çš„é“¾æ¥
                if len(paper_urls) > 1 and len(paper_titles) > 1:
                    links = []
                    for i, (title, url) in enumerate(zip(paper_titles, paper_urls)):
                        links.append(f"[{title}]({url})")
                    paper_source = " & ".join(links)
                else:
                    paper_source = f"[{metric['paper_title']}](" \
                        f"{metric['paper_url']})"

                if metric.get('paper_authors'):
                    paper_source += f" ({metric['paper_authors']})"
            else:
                paper_source = "Internal Implementation"

            # å¤„ç†è¯„æµ‹ç»“æœ
            if metric.get('evaluation_results'):
                # ä¿®æ­£ç›¸å¯¹è·¯å¾„ï¼šä» docs/metrics.md åˆ° docs/eval/prompt/xxx.md
                eval_path = metric['evaluation_results']
                if eval_path.startswith('docs/'):
                    eval_path = eval_path[5:]  # å»æ‰ 'docs/' å‰ç¼€
                eval_results = f"[ğŸ“Š See Results]({eval_path})"
            else:
                eval_results = "N/A"

            table += f"| {type_name} | {metric_name} | {description} | " \
                f"{paper_source} | {eval_results} |\n"

    table += "\n"
    return table


def generate_metrics_documentation() -> str:
    """ç”Ÿæˆå®Œæ•´çš„ metrics æ–‡æ¡£"""
    # æ‰«ææ‰€æœ‰ç±»
    prompt_metrics = scan_prompt_classes()
    rule_metrics = scan_rule_classes()

    # åˆå¹¶æ‰€æœ‰metrics
    all_metrics = prompt_metrics + rule_metrics

    # æŒ‰ç±»åˆ«åˆ†ç»„
    categories = {}
    for metric in all_metrics:
        category = metric.get('category', 'other')
        if category not in categories:
            categories[category] = []
        categories[category].append(metric)

    # ç”Ÿæˆæ–‡æ¡£
    doc = "# Data Quality Metrics\n\n"
    doc += "This document provides comprehensive information about " \
           "all quality metrics used in Dingo.\n\n"
    doc += "**Note**: All metrics are backed by academic sources to " \
           "ensure objectivity and scientific rigor.\n\n"

    # æŒ‰é¢„å®šä¹‰é¡ºåºç”Ÿæˆå„ä¸ªç±»åˆ«
    category_order = ["Text Quality Assessment Metrics", "SFT Data Assessment Metrics",
                      "Classification Metrics", "Multimodality Assessment Metrics",
                      "Rule-Based TEXT Quality Metrics", "Rule-Based IMG Quality Metrics"]

    processed_categories = set()

    # é¦–å…ˆå¤„ç†é¢„å®šä¹‰ç±»åˆ«
    for category in category_order:
        if category in categories:
            doc += generate_table_section(category, categories[category])
            processed_categories.add(category)

    # å¤„ç†æœªé¢„å®šä¹‰çš„ç±»åˆ« - å½’å…¥"other"æˆ–å•ç‹¬æ˜¾ç¤º
    unprocessed_categories = set(categories.keys()) - processed_categories - {"other"}

    if unprocessed_categories:
        # å¦‚æœæœ‰æœªé¢„å®šä¹‰çš„ç±»åˆ«ï¼Œå…ˆæ˜¾ç¤ºå®ƒä»¬
        for category in sorted(unprocessed_categories):
            doc += generate_table_section(category, categories[category])
            processed_categories.add(category)

    # æœ€åå¤„ç†æ˜¾å¼çš„"other"ç±»åˆ«ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "other" in categories:
        doc += generate_table_section("Other Metrics", categories["other"])

    return doc


def main():
    """ä¸»å‡½æ•°"""
    try:
        documentation = generate_metrics_documentation()

        # å†™å…¥æ–‡æ¡£æ–‡ä»¶
        output_file = project_root / "docs" / "metrics.md"
        output_file.parent.mkdir(exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(documentation)

        print(f"âœ… Metrics documentation generated successfully: {output_file}")

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        prompt_metrics = scan_prompt_classes()
        rule_metrics = scan_rule_classes()
        all_metrics = prompt_metrics + rule_metrics

        print(f"ğŸ“Š Total metrics found: {len(all_metrics)}")
        print(f"   - Prompt-based: {len(prompt_metrics)}")
        print(f"   - Rule-based: {len(rule_metrics)}")

        categories = {}
        for metric in all_metrics:
            category = metric.get('category', 'other')
            categories[category] = categories.get(category, 0) + 1

        print("ğŸ“ˆ Metrics by category:")
        for category, count in sorted(categories.items()):
            print(f"   - {category}: {count}")

    except Exception as e:
        print(f"âŒ Error generating documentation: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
